# R script of BayesCluster (continuous) with simulated annealing

library(mvnfast)
library(pcaMethods)
library(mvtnorm)


##-------------------------------------------------
## BAYESCLUSTER (CONTINUOUS)-----------------------
##-------------------------------------------------

BayesClusternoNLP_onlySA1ds <- function(dataMatrix, nPcomponents, nClusters, start_temp, C_k, alpha, nIterations, tol = 1e-3){
  nDataItems  <- nrow(dataMatrix)                                           # number of data points we have
  nFeatures   <- ncol(dataMatrix)                                           # dimension of data 1
  
  Cind        <- matrix(data = NA, nrow = nDataItems, ncol = nIterations)   # cluster allocation matrix
  
  #Cind[,1] <- sample(nClusters, nDataItems,replace = TRUE)                 # randomly assign data points to clusters
  Cind[,1] <- kmeans(dataMatrix, centers = nClusters)$cluster               # using k-means initialisation
 
  mean.ppca<- matrix(data = NA, nrow = nDataItems, ncol = nPcomponents)    # matrix to store the means of the clusters
  cov.ppca <- list()                                                       # list to store the covariance matrices of the clusters
  nClPoints<- c()                                                          # empty vector to store the number of points in each cluster
  max.logposterior <- -Inf                                                 # the highest log posterior - initialised to be 0
  logposterior     <- c()                                                  # the log posterior at the current iteration
  max.partition    <- c()                                                  # keep the partition which corresponds to the highest log posterior
  
  ### INITIALISATION ####
  dm.ppca <- pca(dataMatrix, nPcs = nPcomponents, method = "ppca", scale = "none", center = FALSE)
  z        <- dm.ppca@scores
  W1       <- dm.ppca@loadings
  sigma2.1 <- rgamma(1,1,1)
  #sigma2.1 <-  1.5
  C.1      <- W1%*%t(W1) + sigma2.1*diag(1,nFeatures)
  
  
  current_temp    <- c()     # saving the cooling temperature
  current_temp[1] <- start_temp
  
  converged <- c()         # converge indicator vector
  
  # initialise the number of points, mean.ppca and the cov.ppca list to keep them updated
  for (i in 1:nClusters){
    mean.ppca[i,]   <- colMeans(z[which(Cind[,1]==i),])
    cov.ppca[[i]]   <- cov(z[which(Cind[,1]==i),])
    nClPoints[i]    <- nrow(z[which(Cind[,1]==i),])
  }
  
  # lists/matrices to store the MH updates
  z.chain       <- matrix(data = NA, nrow = nIterations*nDataItems, ncol = nPcomponents)     # latent variables
  mean.chain    <- matrix(data = NA, nrow = nIterations*nDataItems, ncol = nPcomponents)     # cluster means
  W1.chain      <- list()     # loadings 
  sigma21.chain <- c()        # error terms 
 
  # fill in the MH chains
  z.chain[1:nDataItems,]     <- z
  mean.chain[1:nDataItems,]  <- mean.ppca[1:nDataItems,]
  W1.chain[[1]]              <- W1
  sigma21.chain[1]           <- sigma2.1
  
  
  for (i in 2:nIterations){
    #i <- 2 # initialise the iteration counter
    #while((!converged) & (i < nIterations)) { 
    #cat("Starting sample ", i, "...", fill=TRUE)
    
    # Step 1: sample a random permutation of {1,..., nDataItems} 
    tau <- sample(1:nDataItems, nDataItems, replace = FALSE)
    
    
    # Step 2: resample the cluster indicators
    for (j in tau){
      # remove the sufficient statistics of z[j,] from the old cluster + change the number of points in the cluster from which we remove z[j,]
      if (nClPoints[Cind[j,(i-1)]]>2){
        cov.ppca[[Cind[j,i-1]]]  <- cov.new(cov.ppca[[Cind[j,i-1]]], mean.ppca[Cind[j,i-1],], nClPoints[Cind[j,i-1]], z[j,])
        mean.ppca[Cind[j,i-1],]  <- mean.new(mean.ppca[Cind[j,i-1],],nClPoints[Cind[j,i-1]], z[j,])
        nClPoints[Cind[j,i-1]]   <- nClPoints[Cind[j,i-1]] - 1
      } else {   
        if (nClPoints[Cind[j,i-1]]==1){
          nClPoints[Cind[j,i-1]] <- nClPoints[Cind[j,i-1]] - 1
          mean.ppca[Cind[j,i-1],]<- rep(NA, nPcomponents)
          cov.ppca[[Cind[j,i-1]]]<- NA
        } 
        else { # if there are two data points
          if (nClPoints[Cind[j,i-1]]==2){
            cov.ppca[[Cind[j,i-1]]]  <- diag(1,nPcomponents)
            mean.ppca[Cind[j,i-1],]  <- mean.new(mean.ppca[Cind[j,i-1],],nClPoints[Cind[j,i-1]], z[j,])
            nClPoints[Cind[j,i-1]]   <- nClPoints[Cind[j,i-1]] - 1
          }
        } 
      }
      
      # create empty vectors for the prior and predictive probabilities of joining an existent cluster
      logprior.ex     <- rep(NA, length(nClPoints))
      pr.loglikelihood<- rep(NA, length(nClPoints))
      
      for (k in which(nClPoints>0)) {
        
        # compute the predictive likelihood of joining existent cluster 
        pr.loglikelihood[k]    <- dmvnorm(z[j,], mean.ppca[k,] , sigma = diag(1,nPcomponents), log = TRUE)+ dmvnorm(dataMatrix[j,], z[j,]%*%t(W1), sigma = sigma2.1*diag(1,nFeatures), log = TRUE)
        
        # compute the prior of joining existent cluster 
        logprior.ex[k]       <- log(nClPoints[k]/(alpha+nDataItems-1))
      }   
      
      # create an empty vector to store posterior probabilities
      logposterior.prob <- rep(NA, length(nClPoints)+1)
      
      # compute the predictive likelihood of joining a new cluster
      prnew.loglikelihood <- dmvnorm(z[j,], rep(0,nPcomponents), sigma = diag(1,nPcomponents), log = TRUE)+dmvnorm(dataMatrix[j,], z[j,]%*%t(W1), sigma = sigma2.1*diag(1,nFeatures), log = TRUE)
      
      # compute the prior of joining a new cluster 
      logprior.new         <- log(alpha/(alpha+nDataItems-1))
      
      # normalise p(c_i|.)  
      for (m in which(nClPoints>0)){
        logposterior.prob[m]<- logprior.ex[m] + pr.loglikelihood[m]
      }
      
      logposterior.prob[length(nClPoints)+1] <- logprior.new + prnew.loglikelihood
      
      # sample from the posterior - add to the most likely cluster
      Cind[j,i]  <- which.max(logposterior.prob)
      
      if (Cind[j,i]==(length(nClPoints)+1)) {         # that's the increased number of clusters 
        Cind[j,i] = which.min(c(nClPoints, 0))  # add it to the first cluster without any data points
        
        # if we start a new cluster, the mean = observation, cov = identity matrix, nClPoints = 1
        cov.ppca[[Cind[j,i]]]   <- diag(1, nPcomponents)
        mean.ppca[Cind[j,i],]   <- z[j,]
        nClPoints[Cind[j,i]]    <- 1
      } else {
        # add z[j,] sufficient statistics for an existing cluster + increase the number of data points in the corresponding cluster
        cov.ppca[[Cind[j,i]]] <- cov.add(cov.ppca[[Cind[j,i]]], mean.ppca[Cind[j,i],], nClPoints[Cind[j,i]], z[j,])
        mean.ppca[Cind[j,i],] <- mean.add(mean.ppca[Cind[j,i],], nClPoints[Cind[j,i]], z[j,])
        nClPoints[Cind[j,i]]  <- nClPoints[Cind[j,i]] + 1
      }
    }
    
    
    
    clustersWithPoints = which(nClPoints>0)
    current_temp[i] <- current_temp[i-1]*C_k
    #current_temp[i] <- 1/log(i)   # logarithmic
    #current_temp[i]  <- 0.95^i    # exponential
    
    prop.update <- MH_cont_SA1ds(z, current_temp[i],  dataMatrix, nPcomponents,Cind[,i], clustersWithPoints, W1,  sigma2.1, C.1,length(clustersWithPoints), mean.ppca[clustersWithPoints,], alpha, nClPoints[clustersWithPoints])
    z           <- prop.update$zs
    W1          <- prop.update$W1
    mean.ppca[clustersWithPoints,] <- prop.update$means
    sigma2.1    <- prop.update$sigma2.1
    
    
    # record the updates of the parameters
    z.chain[(nDataItems*(i-1)+1):(nDataItems*i),]    <- z
    mean.chain[(nDataItems*(i-1)+1):(nDataItems*i),] <- mean.ppca
    W1.chain[[i-1]]     <- W1
    sigma21.chain[i-1]  <- sigma2.1
    
    clustersWithPoints = which(nClPoints>0, arr.ind = TRUE)
    
    
    
    
    logposterior[i-1] <- logposterior_BC_1ds(dataMatrix, nPcomponents, z, Cind[,i], clustersWithPoints, W1, sigma2.1, C.1,  length(clustersWithPoints), mean.ppca[clustersWithPoints,],  alpha, nClPoints[clustersWithPoints])
    if (logposterior[i-1]>= max.logposterior){
      max.logposterior <- logposterior[i-1]
      max.partition    <- Cind[,i]
    }
    
    converged[i-1] = abs(logposterior[i-2] - logposterior[i-1]) <= tol
    
     
    
  }
  
  out = list(clusters = Cind, cluster_points = nClPoints, number_clusters = length(which(nClPoints!=0)), best_partition = max.partition, max_logposterior = max.logposterior, logposteriors= logposterior, chainz = z.chain,  temperatures = current_temp, chaineps1 = sigma21.chain,convergence = converged)
  
}
