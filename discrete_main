# R script of BayesCluster (1x discrete)  + simulated annealing

library(mvnfast)
library(pcaMethods)
library(mvtnorm)
library(klaR)
library(logisticPCA)


##-------------------------------------------------
## BAYESCLUSTER FOR 1 DISC DATASET ----------------
##-------------------------------------------------

BayesCluster_1discds <- function(dataMatrix, nPcomponents, nCategories, nClusters, start_temp, C_k, alpha, nIterations, tol = 0.00001){
  nDataItems  <- nrow(dataMatrix)                                           # number of data points we have
  nFeatures   <- ncol(dataMatrix)                                           # dimension of data 1
  Cind        <- matrix(data = NA, nrow = nDataItems, ncol = nIterations)   # cluster allocation matrix
  
  #Cind[,1] <- sample(nClusters, nDataItems,replace = TRUE)                 # randomly assign data points to clusters
  Cind[,1] <- kmodes(dataMatrix, nClusters)$cluster
  
  #print(Cind[,1])
  #print(dim(Cind[,1]))
  #dm.pcamix <- PCAmix(X.quanti = NULL, X.quali = dataMatrix, ndim = nPcomponents, rename.level = TRUE)
  mean.ppca<- matrix(data = NA, nrow = nDataItems, ncol = nPcomponents)    # matrix to store the means of the clusters
  cov.ppca <- list()                                                       # list to store the covariance matrices of the clusters
  nClPoints<- c()                                                          # empty vector to store the number of points in each cluster
  max.logposterior <- -Inf                                                 # the highest log posterior - initialised to be 0
  logposterior     <- c()                                                  # the log posterior at the current iteration
  max.partition    <- c()                                                  # keep the partition which corresponds to the highest log posterior
  
  ### INITIALISATION ####
  #dm.ppca     <- pca(dataMatrix_cont, nPcs = nPcomponents, method = "ppca", scale = "none", center = FALSE)
  #z           <- dm.ppca@scores
 # z <- rmvnorm(nDataItems, mean = rep(0,nPcomponents), sigma = diag(1,nPcomponents))
  #z <- jitter(lat.vars)
 
  #z <- dm.pcamix$ind$coord
  z  <- logisticPCA(dataMatrix, k = nPcomponents, m = 4, main_effects = FALSE)$PCs
  
  #print(dim(z))
  #str(z)
  # create empty list to store the loadings matrices and a matrix to store the offset terms
  W.d <- list()
  mu.d<- list()    
  
  # initialise loadings and offset vectors
  for (r in 1:nFeatures){
    W.d[[r]] <- rmvnorm(nPcomponents, mean = rep(0,nCategories[r]), sigma = diag(1,nCategories[r]))
    mu.d[[r]] <- rmvnorm(1, mean = rep(0,nCategories[r]), sigma  = diag(1,nCategories[r]))
  }
  #print(W.d)
  #print(mu.d)
  
  
  current_temp    <- c()     # saving the cooling temperature
  current_temp[1] <- start_temp
  
  converged <- c()         # converge indicator vector
  
  # initialise the number of points, mean.ppca and the cov.ppca list to keep them updated
  for (i in 1:nClusters){
    mean.ppca[i,]   <- colMeans(as.matrix(z[which(Cind[,1]==i),]))
    cov.ppca[[i]]   <- cov(as.matrix(z[which(Cind[,1]==i),]))
    nClPoints[i]    <- nrow(as.matrix(z[which(Cind[,1]==i),]))
  }
  
  # lists/matrices to store the MH updates
  z.chain       <- matrix(data = NA, nrow = nIterations*nDataItems, ncol = nPcomponents)     # latent variables
  mean.chain    <- matrix(data = NA, nrow = nIterations*nDataItems, ncol = nPcomponents)     # cluster means
  Wd.chain      <- list()
  mud.chain     <- list()
  
  # fill in the MH chains
  z.chain[1:nDataItems,]     <- z
  Wd.chain[[1]]              <- W.d
  mud.chain[[1]]             <- mu.d
 
  
  for (i in 2:nIterations){
    #i <- 2 # initialise the iteration counter
    #while((!converged) & (i < nIterations)) { 
    cat("Starting sample ", i, "...", fill=TRUE)
  
    # Step 1: sample a random permutation of {1,..., nDataItems} 
    tau <- sample(1:nDataItems, nDataItems, replace = FALSE)
    
    
    # Step 2: resample the cluster indicators
    for (j in tau){
      # remove the sufficient statistics of z[j,] from the old cluster + change the number of points in the cluster from which we remove z[j,]
      if (nClPoints[Cind[j,(i-1)]]>2){
        cov.ppca[[Cind[j,i-1]]]  <- cov.new(cov.ppca[[Cind[j,i-1]]], mean.ppca[Cind[j,i-1],], nClPoints[Cind[j,i-1]], z[j,])
        mean.ppca[Cind[j,i-1],]  <- mean.new(mean.ppca[Cind[j,i-1],],nClPoints[Cind[j,i-1]], z[j,])
        nClPoints[Cind[j,i-1]]   <- nClPoints[Cind[j,i-1]] - 1
      } else {   
        if (nClPoints[Cind[j,i-1]]==1){
          nClPoints[Cind[j,i-1]] <- nClPoints[Cind[j,i-1]] - 1
          mean.ppca[Cind[j,i-1],]<- rep(NA, nPcomponents)
          cov.ppca[[Cind[j,i-1]]]<- NA
        } 
        else { # if there are two data points
          if (nClPoints[Cind[j,i-1]]==2){
            cov.ppca[[Cind[j,i-1]]]  <- diag(1,nPcomponents)
            mean.ppca[Cind[j,i-1],]  <- mean.new(mean.ppca[Cind[j,i-1],],nClPoints[Cind[j,i-1]], z[j,])
            nClPoints[Cind[j,i-1]]   <- nClPoints[Cind[j,i-1]] - 1
          }
        } 
      }
      
      # create empty vectors for the prior and predictive probabilities of joining an existent cluster
      logprior.ex     <- rep(NA, length(nClPoints))
      pr.loglikelihood<- rep(NA, length(nClPoints))
      
      softmax.product <- 0   # initialise the sofmax product to be 1
      for (s in 1:nFeatures){
        softmax.product <- softmax.product+log((softmax(z[j,]%*%W.d[[s]]+mu.d[[s]])[dataMatrix[j,s]]))
      }
     
     
      for (k in which(nClPoints>0)) {
        
        pr.loglikelihood[k]    <- dmvnorm(z[j,], mean.ppca[k,] , sigma = diag(1,nPcomponents), log = TRUE)+softmax.product
        
        # compute the prior of joining existent cluster 
        logprior.ex[k]       <- log(nClPoints[k]/(alpha+nDataItems-1))
      }   
    
      # create an empty vector to store posterior probabilities
      logposterior.prob <- rep(NA, length(nClPoints)+1)
      
      # compute the predictive likelihood of joining a new cluster
       prnew.loglikelihood <- dmvnorm(z[j,], rep(0,nPcomponents), sigma = diag(1,nPcomponents), log = TRUE)+ softmax.product
      
      # compute the prior of joining a new cluster 
      logprior.new         <- log(alpha/(alpha+nDataItems-1))
      
      # normalise p(c_i|.)  
      for (m in which(nClPoints>0)){
        logposterior.prob[m]<- logprior.ex[m] + pr.loglikelihood[m]
      }
      
      logposterior.prob[length(nClPoints)+1] <- logprior.new + prnew.loglikelihood
      
      Cind[j,i]  <- which.max(logposterior.prob)
      
      if (Cind[j,i]==(length(nClPoints)+1)) {         # that's the increased number of clusters 
        Cind[j,i] = which.min(c(nClPoints, 0))  # add it to the first cluster without any data points
        
        # if we start a new cluster, the mean = observation, cov = identity matrix, nClPoints = 1
        cov.ppca[[Cind[j,i]]]   <- diag(1, nPcomponents)
        mean.ppca[Cind[j,i],]   <- z[j,]
        nClPoints[Cind[j,i]]    <- 1
      } else {
        # add z[j,] sufficient statistics for an existing cluster + increase the number of data points in the corresponding cluster
        cov.ppca[[Cind[j,i]]] <- cov.add(cov.ppca[[Cind[j,i]]], mean.ppca[Cind[j,i],], nClPoints[Cind[j,i]], z[j,])
        mean.ppca[Cind[j,i],] <- mean.add(mean.ppca[Cind[j,i],], nClPoints[Cind[j,i]], z[j,])
        nClPoints[Cind[j,i]]  <- nClPoints[Cind[j,i]] + 1
      }
    }
    
    
    
    clustersWithPoints = which(nClPoints>0)
    
    current_temp[i] <- current_temp[i-1]*C_k
    #current_temp[i] <- 1/log(i)   # logarithmic
    #current_temp[i]  <- 0.95^i    # exponential
    
    prop.update <- MH_cont_1discds(z, current_temp[i], dataMatrix, nPcomponents, nCategories, Cind[,i], clustersWithPoints, W.d,  mu.d,  length(clustersWithPoints), mean.ppca[clustersWithPoints,], alpha, nClPoints[clustersWithPoints])
    z           <- prop.update$zs
    W.d         <- prop.update$Wd
    mean.ppca[clustersWithPoints,] <- prop.update$means
    mu.d        <- prop.update$offsets
    
    
    # record the updates of the parameters
    z.chain[(nDataItems*(i-1)+1):(nDataItems*i),]    <- z
    Wd.chain[[i-1]]     <- W.d
    mud.chain[[i-1]]    <- mu.d

    
    clustersWithPoints = which(nClPoints>0, arr.ind = TRUE)
    
    
    
    
    logposterior[i-1] <- logposterior_BC_1discds(dataMatrix, nPcomponents, nCategories, z, Cind[,i], clustersWithPoints, W.d,  mu.d,length(clustersWithPoints), mean.ppca[clustersWithPoints,],  alpha, nClPoints[clustersWithPoints])
    if (logposterior[i-1]>= max.logposterior){
      max.logposterior <- logposterior[i-1]
      max.partition    <- Cind[,i]
    }
    
    converged[i-1] = abs(logposterior[i-2] - logposterior[i-1]) <= tol
    
    
    
  }
  
  out = list(clusters = Cind, cluster_points = nClPoints, number_clusters = length(which(nClPoints!=0)), best_partition = max.partition, max_logposterior = max.logposterior, logposteriors= logposterior, chainz = z.chain,  temperatures = current_temp, convergence = converged)
  
}
